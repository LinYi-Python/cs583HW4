{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTyOOLmLYMBp"
   },
   "source": [
    "# Home 4: Build a CNN for image recognition.\n",
    "\n",
    "### Name: [Yi Lin]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VstYBrNHYMBw"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run the code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    * Missing **the output after execution** will not be graded.\n",
    "    \n",
    "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM4/HM4.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
    "\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
    "\n",
    "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
    "\n",
    "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
    "\n",
    "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lv0uShOhYMBx"
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Azz3TC8VYMBy"
   },
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOHhkkIEYMBy",
    "outputId": "22fdd686-05b7-4325-fb2f-787512c98e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_Ya5QwkYMBz"
   },
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YcnrSA4YMBz",
    "outputId": "d01570fe-f250-4943-d597-01f1b30d03fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    one_hot_matrix = numpy.zeros((y.shape[0], num_class))\n",
    "    for i in range(y.shape[0]):\n",
    "        one_hot_numbers = y[i][0]\n",
    "        one_hot_matrix[i][one_hot_numbers] = 1\n",
    "    return one_hot_matrix\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3WTMwpwYMBz"
   },
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5ypgI_dYMBz"
   },
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5fRIuquYMB0",
    "outputId": "6e08e9e8-1b99-4031-a0a3-ba495da5e391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5SNGjV8YMB0"
   },
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VZ4d8qRYMB1"
   },
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbWHWJI9YMB1",
    "outputId": "ba4547bd-00ff-4525-d9a6-7b7f4bfd944b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 2,069,534\n",
      "Trainable params: 2,066,566\n",
      "Non-trainable params: 2,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(300))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GO5s1QKnYMB1"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-5 # to be tuned!\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yjQNMyXYMB2",
    "outputId": "bea20be6-050b-46c0-923b-ba65cf969bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 515s 410ms/step - loss: 3.0131 - acc: 0.1078 - val_loss: 2.5359 - val_acc: 0.1021\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 512s 410ms/step - loss: 2.7155 - acc: 0.1482 - val_loss: 2.5107 - val_acc: 0.1087\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 512s 410ms/step - loss: 2.4918 - acc: 0.1851 - val_loss: 2.4757 - val_acc: 0.1125\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 514s 412ms/step - loss: 2.3325 - acc: 0.2098 - val_loss: 2.4567 - val_acc: 0.1123\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 513s 410ms/step - loss: 2.2037 - acc: 0.2344 - val_loss: 2.4265 - val_acc: 0.1175\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 512s 409ms/step - loss: 2.1079 - acc: 0.2468 - val_loss: 2.3900 - val_acc: 0.1222\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 512s 410ms/step - loss: 2.0155 - acc: 0.2657 - val_loss: 2.3828 - val_acc: 0.1271\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 514s 411ms/step - loss: 1.9316 - acc: 0.2884 - val_loss: 2.3484 - val_acc: 0.1415\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 515s 412ms/step - loss: 1.8804 - acc: 0.3070 - val_loss: 2.2705 - val_acc: 0.1615\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 515s 412ms/step - loss: 1.8162 - acc: 0.3272 - val_loss: 2.2614 - val_acc: 0.1718\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 1.7739 - acc: 0.3490 - val_loss: 2.2679 - val_acc: 0.1824\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 509s 407ms/step - loss: 1.7282 - acc: 0.3667 - val_loss: 2.1602 - val_acc: 0.2090\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 1.6771 - acc: 0.3739 - val_loss: 2.2206 - val_acc: 0.2080\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 512s 410ms/step - loss: 1.6306 - acc: 0.3934 - val_loss: 2.1040 - val_acc: 0.2475\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 511s 409ms/step - loss: 1.6004 - acc: 0.4091 - val_loss: 2.1172 - val_acc: 0.2496\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 509s 407ms/step - loss: 1.5631 - acc: 0.4221 - val_loss: 2.0772 - val_acc: 0.2737\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 508s 406ms/step - loss: 1.5493 - acc: 0.4282 - val_loss: 2.0745 - val_acc: 0.2815\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 509s 407ms/step - loss: 1.5104 - acc: 0.4426 - val_loss: 2.0312 - val_acc: 0.2945\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 506s 405ms/step - loss: 1.4916 - acc: 0.4529 - val_loss: 2.0082 - val_acc: 0.3082\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 506s 405ms/step - loss: 1.4579 - acc: 0.4625 - val_loss: 1.9236 - val_acc: 0.3368\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 1.4348 - acc: 0.4694 - val_loss: 1.9024 - val_acc: 0.3448\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 1.4139 - acc: 0.4866 - val_loss: 1.8972 - val_acc: 0.3492\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 1.3908 - acc: 0.4884 - val_loss: 1.8557 - val_acc: 0.3650\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 511s 408ms/step - loss: 1.3796 - acc: 0.4988 - val_loss: 1.8761 - val_acc: 0.3609\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 1.3645 - acc: 0.5020 - val_loss: 1.7420 - val_acc: 0.3951\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 509s 408ms/step - loss: 1.3434 - acc: 0.5107 - val_loss: 1.7744 - val_acc: 0.3892\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 511s 409ms/step - loss: 1.3235 - acc: 0.5209 - val_loss: 1.8498 - val_acc: 0.3741\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 1.3076 - acc: 0.5248 - val_loss: 1.8319 - val_acc: 0.3871\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 1.2776 - acc: 0.5303 - val_loss: 1.7132 - val_acc: 0.4159\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 513s 410ms/step - loss: 1.2784 - acc: 0.5358 - val_loss: 1.7324 - val_acc: 0.4162\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 512s 410ms/step - loss: 1.2504 - acc: 0.5467 - val_loss: 1.8411 - val_acc: 0.3906\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 512s 409ms/step - loss: 1.2427 - acc: 0.5554 - val_loss: 1.7949 - val_acc: 0.4003\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 512s 410ms/step - loss: 1.2209 - acc: 0.5573 - val_loss: 1.7268 - val_acc: 0.4221\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 513s 410ms/step - loss: 1.2153 - acc: 0.5668 - val_loss: 1.6357 - val_acc: 0.4465\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 1.2066 - acc: 0.5672 - val_loss: 1.6879 - val_acc: 0.4404\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 509s 408ms/step - loss: 1.1894 - acc: 0.5719 - val_loss: 1.5857 - val_acc: 0.4641\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 515s 412ms/step - loss: 1.1729 - acc: 0.5760 - val_loss: 1.6572 - val_acc: 0.4513\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 515s 412ms/step - loss: 1.1582 - acc: 0.5847 - val_loss: 1.5370 - val_acc: 0.4778\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 515s 412ms/step - loss: 1.1539 - acc: 0.5888 - val_loss: 1.5799 - val_acc: 0.4690\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 516s 413ms/step - loss: 1.1387 - acc: 0.5902 - val_loss: 1.5386 - val_acc: 0.4913\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 515s 412ms/step - loss: 1.1314 - acc: 0.5958 - val_loss: 1.6017 - val_acc: 0.4752\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 516s 413ms/step - loss: 1.1107 - acc: 0.6005 - val_loss: 1.5935 - val_acc: 0.4763\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 517s 414ms/step - loss: 1.1213 - acc: 0.5981 - val_loss: 1.5639 - val_acc: 0.4800\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 518s 414ms/step - loss: 1.1054 - acc: 0.6075 - val_loss: 1.5799 - val_acc: 0.4844\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 517s 413ms/step - loss: 1.0885 - acc: 0.6159 - val_loss: 1.5139 - val_acc: 0.4979\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 517s 414ms/step - loss: 1.0847 - acc: 0.6120 - val_loss: 1.5123 - val_acc: 0.5049\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 520s 416ms/step - loss: 1.0737 - acc: 0.6166 - val_loss: 1.5386 - val_acc: 0.5030\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 516s 413ms/step - loss: 1.0494 - acc: 0.6257 - val_loss: 1.5861 - val_acc: 0.4890\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 517s 413ms/step - loss: 1.0513 - acc: 0.6249 - val_loss: 1.4470 - val_acc: 0.5255\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 520s 416ms/step - loss: 1.0367 - acc: 0.6283 - val_loss: 1.4896 - val_acc: 0.5081\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 525s 420ms/step - loss: 1.0349 - acc: 0.6321 - val_loss: 1.4500 - val_acc: 0.5261\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 522s 418ms/step - loss: 1.0283 - acc: 0.6354 - val_loss: 1.6158 - val_acc: 0.4989\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 519s 415ms/step - loss: 1.0097 - acc: 0.6412 - val_loss: 1.4565 - val_acc: 0.5286\n",
      "Epoch 54/100\n",
      "1250/1250 [==============================] - 519s 416ms/step - loss: 1.0019 - acc: 0.6419 - val_loss: 1.5199 - val_acc: 0.5234\n",
      "Epoch 55/100\n",
      "1250/1250 [==============================] - 521s 417ms/step - loss: 0.9764 - acc: 0.6494 - val_loss: 1.5961 - val_acc: 0.5102\n",
      "Epoch 56/100\n",
      "1250/1250 [==============================] - 516s 413ms/step - loss: 0.9923 - acc: 0.6519 - val_loss: 1.5007 - val_acc: 0.5309\n",
      "Epoch 57/100\n",
      "1250/1250 [==============================] - 519s 415ms/step - loss: 0.9802 - acc: 0.6505 - val_loss: 1.4562 - val_acc: 0.5450\n",
      "Epoch 58/100\n",
      "1250/1250 [==============================] - 518s 414ms/step - loss: 0.9609 - acc: 0.6576 - val_loss: 1.5078 - val_acc: 0.5348\n",
      "Epoch 59/100\n",
      "1250/1250 [==============================] - 519s 415ms/step - loss: 0.9774 - acc: 0.6553 - val_loss: 1.4659 - val_acc: 0.5458\n",
      "Epoch 60/100\n",
      "1250/1250 [==============================] - 514s 411ms/step - loss: 0.9625 - acc: 0.6591 - val_loss: 1.6469 - val_acc: 0.5123\n",
      "Epoch 61/100\n",
      "1250/1250 [==============================] - 513s 410ms/step - loss: 0.9671 - acc: 0.6607 - val_loss: 1.4151 - val_acc: 0.5536\n",
      "Epoch 62/100\n",
      "1250/1250 [==============================] - 514s 411ms/step - loss: 0.9491 - acc: 0.6677 - val_loss: 1.3964 - val_acc: 0.5632\n",
      "Epoch 63/100\n",
      "1250/1250 [==============================] - 513s 410ms/step - loss: 0.9481 - acc: 0.6666 - val_loss: 1.4135 - val_acc: 0.5632\n",
      "Epoch 64/100\n",
      "1250/1250 [==============================] - 513s 410ms/step - loss: 0.9360 - acc: 0.6671 - val_loss: 1.4783 - val_acc: 0.5443\n",
      "Epoch 65/100\n",
      "1250/1250 [==============================] - 514s 411ms/step - loss: 0.9214 - acc: 0.6756 - val_loss: 1.3900 - val_acc: 0.5636\n",
      "Epoch 66/100\n",
      "1250/1250 [==============================] - 511s 409ms/step - loss: 0.9237 - acc: 0.6737 - val_loss: 1.5217 - val_acc: 0.5474\n",
      "Epoch 67/100\n",
      "1250/1250 [==============================] - 508s 406ms/step - loss: 0.9182 - acc: 0.6733 - val_loss: 1.4013 - val_acc: 0.5698\n",
      "Epoch 68/100\n",
      "1250/1250 [==============================] - 512s 410ms/step - loss: 0.9069 - acc: 0.6785 - val_loss: 1.3782 - val_acc: 0.5818\n",
      "Epoch 69/100\n",
      "1250/1250 [==============================] - 507s 406ms/step - loss: 0.9054 - acc: 0.6793 - val_loss: 1.3720 - val_acc: 0.5824\n",
      "Epoch 70/100\n",
      "1250/1250 [==============================] - 508s 406ms/step - loss: 0.8916 - acc: 0.6857 - val_loss: 1.4024 - val_acc: 0.5736\n",
      "Epoch 71/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 0.9072 - acc: 0.6788 - val_loss: 1.4043 - val_acc: 0.5819\n",
      "Epoch 72/100\n",
      "1250/1250 [==============================] - 513s 410ms/step - loss: 0.8944 - acc: 0.6871 - val_loss: 1.3554 - val_acc: 0.5923\n",
      "Epoch 73/100\n",
      "1250/1250 [==============================] - 511s 409ms/step - loss: 0.8796 - acc: 0.6900 - val_loss: 1.3241 - val_acc: 0.6012\n",
      "Epoch 74/100\n",
      "1250/1250 [==============================] - 511s 409ms/step - loss: 0.8830 - acc: 0.6896 - val_loss: 1.3778 - val_acc: 0.5862\n",
      "Epoch 75/100\n",
      "1250/1250 [==============================] - 506s 405ms/step - loss: 0.8651 - acc: 0.6970 - val_loss: 1.3198 - val_acc: 0.6003\n",
      "Epoch 76/100\n",
      "1250/1250 [==============================] - 508s 406ms/step - loss: 0.8786 - acc: 0.6910 - val_loss: 1.4223 - val_acc: 0.5793\n",
      "Epoch 77/100\n",
      "1250/1250 [==============================] - 512s 409ms/step - loss: 0.8570 - acc: 0.7000 - val_loss: 1.3487 - val_acc: 0.5970\n",
      "Epoch 78/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 0.8663 - acc: 0.6965 - val_loss: 1.2715 - val_acc: 0.6145\n",
      "Epoch 79/100\n",
      "1250/1250 [==============================] - 510s 408ms/step - loss: 0.8502 - acc: 0.7018 - val_loss: 1.2697 - val_acc: 0.6149\n",
      "Epoch 80/100\n",
      "1250/1250 [==============================] - 511s 409ms/step - loss: 0.8467 - acc: 0.7057 - val_loss: 1.3111 - val_acc: 0.6085\n",
      "Epoch 81/100\n",
      "1250/1250 [==============================] - 498s 399ms/step - loss: 0.8541 - acc: 0.7008 - val_loss: 1.4283 - val_acc: 0.5854\n",
      "Epoch 82/100\n",
      "1250/1250 [==============================] - 501s 401ms/step - loss: 0.8310 - acc: 0.7076 - val_loss: 1.2185 - val_acc: 0.6270\n",
      "Epoch 83/100\n",
      "1250/1250 [==============================] - 508s 407ms/step - loss: 0.8362 - acc: 0.7065 - val_loss: 1.1395 - val_acc: 0.6467\n",
      "Epoch 84/100\n",
      "1250/1250 [==============================] - 513s 410ms/step - loss: 0.8296 - acc: 0.7084 - val_loss: 1.2438 - val_acc: 0.6268\n",
      "Epoch 85/100\n",
      "1250/1250 [==============================] - 513s 411ms/step - loss: 0.8218 - acc: 0.7142 - val_loss: 1.2093 - val_acc: 0.6351\n",
      "Epoch 86/100\n",
      "1250/1250 [==============================] - 516s 413ms/step - loss: 0.8216 - acc: 0.7137 - val_loss: 1.3067 - val_acc: 0.6113\n",
      "Epoch 87/100\n",
      "1250/1250 [==============================] - 515s 412ms/step - loss: 0.8086 - acc: 0.7161 - val_loss: 1.2492 - val_acc: 0.6211\n",
      "Epoch 88/100\n",
      "1250/1250 [==============================] - 515s 412ms/step - loss: 0.8070 - acc: 0.7213 - val_loss: 1.2503 - val_acc: 0.6277\n",
      "Epoch 89/100\n",
      "1250/1250 [==============================] - 519s 416ms/step - loss: 0.7988 - acc: 0.7208 - val_loss: 1.1876 - val_acc: 0.6455\n",
      "Epoch 90/100\n",
      "1250/1250 [==============================] - 519s 415ms/step - loss: 0.8137 - acc: 0.7156 - val_loss: 1.3040 - val_acc: 0.6176\n",
      "Epoch 91/100\n",
      "1250/1250 [==============================] - 518s 414ms/step - loss: 0.8025 - acc: 0.7204 - val_loss: 1.2387 - val_acc: 0.6299\n",
      "Epoch 92/100\n",
      "1250/1250 [==============================] - 515s 412ms/step - loss: 0.8000 - acc: 0.7239 - val_loss: 1.1054 - val_acc: 0.6579\n",
      "Epoch 93/100\n",
      "1250/1250 [==============================] - 513s 410ms/step - loss: 0.7912 - acc: 0.7227 - val_loss: 1.2127 - val_acc: 0.6356\n",
      "Epoch 94/100\n",
      "1250/1250 [==============================] - 513s 410ms/step - loss: 0.7727 - acc: 0.7308 - val_loss: 1.1869 - val_acc: 0.6412\n",
      "Epoch 95/100\n",
      "1250/1250 [==============================] - 513s 410ms/step - loss: 0.7889 - acc: 0.7211 - val_loss: 1.2257 - val_acc: 0.6369\n",
      "Epoch 96/100\n",
      "1250/1250 [==============================] - 512s 410ms/step - loss: 0.7698 - acc: 0.7340 - val_loss: 1.3414 - val_acc: 0.6082\n",
      "Epoch 97/100\n",
      "1250/1250 [==============================] - 512s 409ms/step - loss: 0.7752 - acc: 0.7284 - val_loss: 1.2127 - val_acc: 0.6394\n",
      "Epoch 98/100\n",
      "1250/1250 [==============================] - 512s 410ms/step - loss: 0.7637 - acc: 0.7321 - val_loss: 1.1706 - val_acc: 0.6499\n",
      "Epoch 99/100\n",
      "1250/1250 [==============================] - 512s 410ms/step - loss: 0.7656 - acc: 0.7309 - val_loss: 1.1100 - val_acc: 0.6603\n",
      "Epoch 100/100\n",
      "1250/1250 [==============================] - 511s 409ms/step - loss: 0.7623 - acc: 0.7318 - val_loss: 1.1797 - val_acc: 0.6507\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr, batch_size=32, epochs=100, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "tVaIuRnSYMB2",
    "outputId": "47d0d8b2-1baf-4192-8cbb-473a50717e7e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eltx0hAbYMB3"
   },
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOKF5JXhYMB3"
   },
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bi8Hayu3YMB3",
    "outputId": "65057353-b0f5-410c-a721-f9760bf5328f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 2,069,534\n",
      "Trainable params: 2,066,566\n",
      "Non-trainable params: 2,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# <Compile your model again (using the same hyper-parameters)>\n",
    "# ...\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(300))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "learning_rate = 1E-5 # to be tuned!\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vvSFYGJYMB4",
    "outputId": "ab1d7551-b351-4808-f160-325eda4f3084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 141s 91ms/step - loss: 2.8704 - acc: 0.1248\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 2.5621 - acc: 0.1682\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 141s 90ms/step - loss: 2.3514 - acc: 0.2074\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 141s 90ms/step - loss: 2.1897 - acc: 0.2387\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 142s 91ms/step - loss: 2.0786 - acc: 0.2626\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 143s 91ms/step - loss: 1.9729 - acc: 0.2828\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 142s 91ms/step - loss: 1.8877 - acc: 0.3107\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 140s 90ms/step - loss: 1.8125 - acc: 0.3317\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 142s 91ms/step - loss: 1.7521 - acc: 0.3524\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 143s 92ms/step - loss: 1.6961 - acc: 0.3745\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 1.6496 - acc: 0.3868\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 1.6095 - acc: 0.4002\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 140s 90ms/step - loss: 1.5679 - acc: 0.4189\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 138s 88ms/step - loss: 1.5311 - acc: 0.4343\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 138s 88ms/step - loss: 1.5053 - acc: 0.4466\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 1.4671 - acc: 0.4588\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 1.4377 - acc: 0.4711\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 1.4090 - acc: 0.4838\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 1.3828 - acc: 0.4934\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 1.3528 - acc: 0.5054\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 1.3277 - acc: 0.5159\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 1.3027 - acc: 0.5267\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 1.2805 - acc: 0.5367\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 142s 91ms/step - loss: 1.2578 - acc: 0.5449\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 1.2398 - acc: 0.5514\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 140s 90ms/step - loss: 1.2231 - acc: 0.5598\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 141s 90ms/step - loss: 1.2036 - acc: 0.5653\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 142s 91ms/step - loss: 1.1880 - acc: 0.5739\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 141s 90ms/step - loss: 1.1768 - acc: 0.5772\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 1.1623 - acc: 0.5836\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 144s 92ms/step - loss: 1.1445 - acc: 0.5890\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 151s 97ms/step - loss: 1.1295 - acc: 0.5937\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 1.1200 - acc: 0.5994\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 144s 92ms/step - loss: 1.1081 - acc: 0.60512s - l\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 1.0931 - acc: 0.6080\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 1.0861 - acc: 0.61133s - loss: 1.0861 - a - ETA: 2s - ETA: 0s - loss: 1.0862 - acc: 0.61\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 144s 92ms/step - loss: 1.0740 - acc: 0.6145\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 144s 92ms/step - loss: 1.0624 - acc: 0.6220\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 1.0541 - acc: 0.6249\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 1.0444 - acc: 0.6284\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 143s 92ms/step - loss: 1.0332 - acc: 0.6334\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 1.0220 - acc: 0.6363\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 1.0118 - acc: 0.6431\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 1.0062 - acc: 0.64420s - loss: 1.0066 - acc:\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.9948 - acc: 0.6467\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.9906 - acc: 0.6507\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.9780 - acc: 0.6540\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 149s 95ms/step - loss: 0.9696 - acc: 0.6579\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.9602 - acc: 0.6630\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.9548 - acc: 0.6629\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.9419 - acc: 0.6683\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.9333 - acc: 0.6719\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 146s 94ms/step - loss: 0.9288 - acc: 0.67410s - loss: 0.9287 - acc:\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.9232 - acc: 0.6760\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.9229 - acc: 0.6761\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.9125 - acc: 0.6788\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.9039 - acc: 0.6843\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 146s 94ms/step - loss: 0.8983 - acc: 0.6860\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 146s 94ms/step - loss: 0.8925 - acc: 0.68712s\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 148s 95ms/step - loss: 0.8815 - acc: 0.6908\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.8783 - acc: 0.6915\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 144s 92ms/step - loss: 0.8691 - acc: 0.6962\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.8594 - acc: 0.7014\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.8591 - acc: 0.7012\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.8519 - acc: 0.7013\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.8452 - acc: 0.7034\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.8431 - acc: 0.7056\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.8392 - acc: 0.7053\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 145s 92ms/step - loss: 0.8312 - acc: 0.7110\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.8244 - acc: 0.71180s - loss: 0.8243 - \n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.8160 - acc: 0.7144\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.8122 - acc: 0.7168\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.8096 - acc: 0.7172\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.8004 - acc: 0.7198\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 146s 94ms/step - loss: 0.8051 - acc: 0.7204\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 148s 95ms/step - loss: 0.7933 - acc: 0.7240\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.7914 - acc: 0.7242\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 146s 94ms/step - loss: 0.7816 - acc: 0.7288\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.7800 - acc: 0.7284\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.7758 - acc: 0.7311\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.7744 - acc: 0.7303\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 146s 94ms/step - loss: 0.7620 - acc: 0.7367\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.7623 - acc: 0.7345\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 144s 92ms/step - loss: 0.7589 - acc: 0.7360\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.7501 - acc: 0.7403\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 146s 93ms/step - loss: 0.7528 - acc: 0.7387\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.7520 - acc: 0.7397\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 146s 94ms/step - loss: 0.7414 - acc: 0.7418\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 144s 92ms/step - loss: 0.7400 - acc: 0.7438\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.7311 - acc: 0.7487\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 148s 94ms/step - loss: 0.7292 - acc: 0.7464\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.7244 - acc: 0.7486\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.7181 - acc: 0.7514\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.7203 - acc: 0.7500\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.7123 - acc: 0.7541\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 145s 92ms/step - loss: 0.7058 - acc: 0.7553\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.7044 - acc: 0.7562\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 145s 93ms/step - loss: 0.6996 - acc: 0.7582\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.7003 - acc: 0.7558\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.6908 - acc: 0.7616\n"
     ]
    }
   ],
   "source": [
    "# <Train your model on the entire training set (50K samples)>\n",
    "# <Use (x_train, y_train_vec) instead of (x_tr, y_tr)>\n",
    "# <Do NOT use the validation_data option (because now you do not have validation data)>\n",
    "# ...\n",
    "history = model.fit(x_train, y_train_vec, batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YmSoLtfYMB5"
   },
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "41VBj8K8YMB5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 16ms/step - loss: 1.1008 - acc: 0.6725\n",
      "loss = 1.100770115852356\n",
      "accuracy = 0.6725000143051147\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ozi0MXL2YMB5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HM4-YiLin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
